[{"title":"大模型+强化学习调研","date":"2025-05-31T14:56:14.955Z","url":"/2025/05/31/test/","categories":[[" ",""]],"content":"大模型+强化学习调研目前来看，大模型更擅长解决生成式的问题，但是对于决策性的问题，大模型的处理比较差，比如机器人导航转向就是一个决策性的问题。生成问题通常使用掩码来隐藏上下文信息，让模型通过上文生成下文，这是一种自监督方法；而决策问题通常需要一个明确的答案，如是或否、A&#x2F;B&#x2F;C 选项，因此需要使用有监督数据进行训练或微调模型。 强化学习通过奖励函数直接或间接地为模型提供有监督的判定标准。因此，在大模型中引入强化学习可以提升其判断能力。 RLHFEN：Deep Reinforcement Learning from Human Preferences CH：从人类偏好中进行深度强化学习 论文链接 机构：openai 这种方法主要依赖于人类标注的偏好，即确定人类更倾向于接受机器人的 A 回答还是 B 回答。当时，大模型还未出现，这种方法被用于游戏和机器人环境中，旨在改进强化学习的算法。该算法的训练数据来自人类的标注，提供问题和两个选项，让人类选择更倾向于 A 或 B，或者两者都同样喜欢，或者无法做出判断，从而构建有监督的数据。 后来出现了大模型，人们使用 RLHF 来训练这些大模型，以提升它们学习人类偏好的能力。 RLAIFEN: RLAIF: Scaling Reinforcement Learning from Human Feedback with AI FeedbackCH: RLAIF：利用AI反馈扩展强化学习 论文链接 机构：Google Research 通过机器自动标注偏好数据来减少人力成本，研究者使用 PaLM2 对偏好进行标注，并将训练奖励模型的标注学习与直接让模型给出 0-10 分进行了比较。 传统的标注只是选择两个答案中哪个更好，并没有体现出好坏程度是 9:1 还是 6:4；而大模型有能力可以给出一定的分值。 PaLM2：是谷歌开发的一种先进的大规模语言模型，PaLM 2的设计旨在提升多语言处理能力、逻辑推理能力和代码理解能力。 SPOEN: A Minimaximalist Approach to Reinforcement Learning from Human FeedbackCH: 一种极简极大化的强化学习方法：来自人类反馈的学习 论文链接 机构：Google Research 上述方法可能隐含着 A&gt;B、B&gt;C，可推出 A&gt;C 的逻辑，而实际上在石头剪刀布这类游戏中，A&gt;C 可能并不成立。 该方法不再将选择 A 与 B 相比较，而是同时考虑选项 A 和其他多个方法（BCDE…），计算 A 在这些方法中的位置作为其相对客观分值，以更准确地学习选项的好坏，并做出更好的决策。 LLM-enhanced RLEN：Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods CH：大语言模型增强强化学习综述：概念、分类和方法 论文链接： LLM作为信息处理者大模型为强化学习代理提取观测表征和规范语言，提高样本利用效率。 LLM作为奖励设计者在复杂或无法量化的任务中，大模型利用知识和推理能力设计复杂奖励函数和生成奖励信号。 LLM作为决策者大模型直接生成动作或间接生成动作建议，提高强化学习探索效率。 LLM作为生成者大模型被用于：（1）作为高保真多模态世界模型减少现实世界学习成本及（2）生成代理行为的自然语言解释。 强化学习 DQN流程： Deepseek微调(SFT)参考网站： conda环境配置 报错与解决 在运行之前可以先进行swanlab的安装，详细见blog deepseek预训练模型下载预训练采用deepseek-llm-7b-chat，huggingface地址 其中，需要用到git lfs，在centos 8服务器上的安装方式无法采用命令行安装，可以下载源代码安装，首先从git lfs的github仓库中选择下载 通过在服务器命令行中输入uname -m查询系统架构x86_64，选择Linux AMD64版本进行下载，解压完成后进入文件夹，执行sudo ./install.sh即可，验证是否成功安装可以使用 由于服务器无法连接外网问题，这里不使用命令行克隆，使用程序进行下载，下载代码如下： 数据集下载github链接： 下载其datasets文件夹即可。 微调训练代码 对于数据处理函数，由于使用的数据集不同，其数据集中的格式也有所不同，所以需要修改其数据处理函数能够正确处理数据集为标准输入，关于数据处理函数的修改可交给大模型帮助完成。 模型合并代码进行推理之前需要先将微调生成的模型与原模型进行合并，以下是合并模型代码： 推理代码 测试结果"},{"title":"大模型+强化学习调研","date":"2025-05-28T06:33:57.606Z","url":"/2025/05/28/%E5%A4%A7%E6%A8%A1%E5%9E%8B+%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/","categories":[[" ",""]],"content":"大模型+强化学习调研目前来看，大模型更擅长解决生成式的问题，但是对于决策性的问题，大模型的处理比较差，比如机器人导航转向就是一个决策性的问题。生成问题通常使用掩码来隐藏上下文信息，让模型通过上文生成下文，这是一种自监督方法；而决策问题通常需要一个明确的答案，如是或否、A&#x2F;B&#x2F;C 选项，因此需要使用有监督数据进行训练或微调模型。 强化学习通过奖励函数直接或间接地为模型提供有监督的判定标准。因此，在大模型中引入强化学习可以提升其判断能力。 RLHFEN：Deep Reinforcement Learning from Human Preferences CH：从人类偏好中进行深度强化学习 论文链接 机构：openai 这种方法主要依赖于人类标注的偏好，即确定人类更倾向于接受机器人的 A 回答还是 B 回答。当时，大模型还未出现，这种方法被用于游戏和机器人环境中，旨在改进强化学习的算法。该算法的训练数据来自人类的标注，提供问题和两个选项，让人类选择更倾向于 A 或 B，或者两者都同样喜欢，或者无法做出判断，从而构建有监督的数据。 后来出现了大模型，人们使用 RLHF 来训练这些大模型，以提升它们学习人类偏好的能力。 RLAIFEN: RLAIF: Scaling Reinforcement Learning from Human Feedback with AI FeedbackCH: RLAIF：利用AI反馈扩展强化学习 论文链接 机构：Google Research 通过机器自动标注偏好数据来减少人力成本，研究者使用 PaLM2 对偏好进行标注，并将训练奖励模型的标注学习与直接让模型给出 0-10 分进行了比较。 传统的标注只是选择两个答案中哪个更好，并没有体现出好坏程度是 9:1 还是 6:4；而大模型有能力可以给出一定的分值。 PaLM2：是谷歌开发的一种先进的大规模语言模型，PaLM 2的设计旨在提升多语言处理能力、逻辑推理能力和代码理解能力。 SPOEN: A Minimaximalist Approach to Reinforcement Learning from Human FeedbackCH: 一种极简极大化的强化学习方法：来自人类反馈的学习 论文链接 机构：Google Research 上述方法可能隐含着 A&gt;B、B&gt;C，可推出 A&gt;C 的逻辑，而实际上在石头剪刀布这类游戏中，A&gt;C 可能并不成立。 该方法不再将选择 A 与 B 相比较，而是同时考虑选项 A 和其他多个方法（BCDE…），计算 A 在这些方法中的位置作为其相对客观分值，以更准确地学习选项的好坏，并做出更好的决策。 LLM-enhanced RLEN：Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods CH：大语言模型增强强化学习综述：概念、分类和方法 论文链接： LLM作为信息处理者大模型为强化学习代理提取观测表征和规范语言，提高样本利用效率。 LLM作为奖励设计者在复杂或无法量化的任务中，大模型利用知识和推理能力设计复杂奖励函数和生成奖励信号。 LLM作为决策者大模型直接生成动作或间接生成动作建议，提高强化学习探索效率。 LLM作为生成者大模型被用于：（1）作为高保真多模态世界模型减少现实世界学习成本及（2）生成代理行为的自然语言解释。 强化学习 DQN流程： Deepseek微调(SFT)参考网站： conda环境配置 报错与解决 在运行之前可以先进行swanlab的安装，详细见blog deepseek预训练模型下载预训练采用deepseek-llm-7b-chat，huggingface地址 其中，需要用到git lfs，在centos 8服务器上的安装方式无法采用命令行安装，可以下载源代码安装，首先从git lfs的github仓库中选择下载 通过在服务器命令行中输入uname -m查询系统架构x86_64，选择Linux AMD64版本进行下载，解压完成后进入文件夹，执行sudo ./install.sh即可，验证是否成功安装可以使用 由于服务器无法连接外网问题，这里不使用命令行克隆，使用程序进行下载，下载代码如下： 数据集下载github链接： 下载其datasets文件夹即可。 微调训练代码 对于数据处理函数，由于使用的数据集不同，其数据集中的格式也有所不同，所以需要修改其数据处理函数能够正确处理数据集为标准输入，关于数据处理函数的修改可交给大模型帮助完成。 模型合并代码进行推理之前需要先将微调生成的模型与原模型进行合并，以下是合并模型代码： 推理代码 测试结果"}]